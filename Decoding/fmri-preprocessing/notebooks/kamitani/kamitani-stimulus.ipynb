{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ab70a6-c99d-4689-a5f8-b3b64466d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from tqdm.notebook import tqdm\n",
    "import nibabel as nib\n",
    "import glmsingle\n",
    "from glmsingle.glmsingle import GLM_single\n",
    "import bids\n",
    "from bids import BIDSLayout\n",
    "from scipy.ndimage import zoom, binary_dilation\n",
    "import h5py\n",
    "import nibabel as nib\n",
    "from einops import rearrange\n",
    "\n",
    "dir2 = os.path.abspath('..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc7defe-85d3-4b94-bc4c-505c2888156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the path to the Kamitani dataset\n",
    "derivatives_path = Path('X:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\')\n",
    "derivatives_path_ssd = Path('D:\\\\Datasets\\\\Deep-Image-Reconstruction\\\\derivatives\\\\')\n",
    "dataset_path = derivatives_path / 'fmriprep-20.2.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8ddf81-2cd6-49db-99ee-477d1c4b9a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "stimulus_images = h5py.File(derivatives_path / 'stimulus_images.hdf5', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbbc1a59-ecd7-497d-ac90-6f3949dc2d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
     ]
    }
   ],
   "source": [
    "# Load a CLIP model\n",
    "import clip\n",
    "\n",
    "print(clip.available_models())\n",
    "model_name = 'ViT-B/32'\n",
    "model, preprocess = clip.load(model_name, device=device)\n",
    "model = model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "badde157-1c8f-4c92-881b-08a352e2380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features to save\n",
    "\n",
    "save_modules = {\n",
    "    '': 'embedding'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e2a2c47-0f98-4f43-b1fe-ff6187cb4461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff284206a0e4baeb0e367162b46c52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Feature extraction\n",
    "\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "from typing import Sequence, Dict\n",
    "\n",
    "modules = dict(model.named_modules())\n",
    "\n",
    "with h5py.File(derivatives_path / f\"{model_name.replace('/', '=')}-features.hdf5\", \"a\") as f:\n",
    "    images = list(enumerate(stimulus_images.items()))\n",
    "    N = len(images)\n",
    "    for i, (stimulus_id, stimulus_image) in tqdm(images):\n",
    "        image_data = stimulus_image['data'][:]\n",
    "        image = Image.fromarray(image_data)\n",
    "        x = preprocess(image).unsqueeze(0).to(device).to(torch.float16)\n",
    "\n",
    "        features = {}\n",
    "        def forward_hook(module_name, module, x_in, x_out):\n",
    "            if x_out.shape[0] == 1:\n",
    "                x_out = x_out[0]\n",
    "            features[module_name] = x_out.clone().cpu().numpy()\n",
    "        \n",
    "        hook_handles = []\n",
    "        if isinstance(save_modules, Sequence):\n",
    "            for module_name in save_modules:\n",
    "                module = modules[module_name]\n",
    "                hook_handle = module.register_forward_hook(partial(forward_hook, module_name))\n",
    "                hook_handles.append(hook_handle)\n",
    "        elif isinstance(save_modules, Dict):\n",
    "            for module_name, feature_name in save_modules.items():\n",
    "                module = modules[module_name]\n",
    "                hook_handle = module.register_forward_hook(partial(forward_hook, feature_name))\n",
    "                hook_handles.append(hook_handle)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model(x)\n",
    "            \n",
    "        for hook_handle in hook_handles:\n",
    "            hook_handle.remove()\n",
    "        \n",
    "        for feature_name, feature in features.items():\n",
    "            f.require_dataset(feature_name, (N, *feature.shape), feature.dtype)\n",
    "            f[feature_name][i] = feature\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri-preprocessing",
   "language": "python",
   "name": "fmri-preprocessing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
