{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfad221c",
   "metadata": {},
   "source": [
    "Importing Libraries:  \n",
    "Python imports and modules that are required that are imported at the start:\n",
    "-\tos, sys, time, numpy (np alias), pandas (pd alias), matplotlib.pyplot (plt alias),  ipywidgets, tqdm.notebook, nibabel, glmsingle, bids, noise_ceiling, tc2see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42692ec7-9a74-481a-9a60-846a52e70f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from tqdm.notebook import tqdm\n",
    "import nibabel as nib\n",
    "# import glmsingle\n",
    "# from glmsingle.glmsingle import GLM_single\n",
    "import bids\n",
    "from bids import BIDSLayout\n",
    "from scipy.ndimage import zoom, binary_dilation\n",
    "import h5py\n",
    "import nibabel as nib\n",
    "from einops import rearrange\n",
    "\n",
    "dir2 = os.path.abspath('..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)\n",
    "    \n",
    "from tc2see import load_data\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from fracridge import FracRidgeRegressorCV\n",
    "from metrics import (\n",
    "    cosine_distance, squared_euclidean_distance, r2_score, two_versus_two,\n",
    "    two_versus_two_slow\n",
    ")\n",
    "import warnings\n",
    "\n",
    "from noise_ceiling import (\n",
    "    compute_ncsnr,\n",
    "    compute_nc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c95e0",
   "metadata": {},
   "source": [
    "Setting Paths and Variables:  \n",
    "It sets up various directory paths for loading and saving data:\n",
    "-\ttc2see_version and subject are variables that specify the version of the dataset and the subject being analyzed.\n",
    "-\ttr is the repetition time (time between volume acquisitions in the fMRI data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a523daaa-43f8-436a-bb98-910bfaf23fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = Path('E:\\\\fmri_processing\\\\results')\n",
    "tc2see_version = 3\n",
    "dataset_path = dataset_root\n",
    "derivatives_path = dataset_path / 'derivatives_TC2See'\n",
    "data_path = derivatives_path / 'fmriprep'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af4e9b",
   "metadata": {},
   "source": [
    "Computing Accuracy, Standard Dev, etc:  \n",
    "This code segment performs a series of operations to evaluate the accuracy of a model's predictions for a given subject. It calculates the accuracy and variance of the model's predictions using a cross-validation approach with a range of parameters. The code initializes various variables, including the subject ID and configuration parameters. It then iterates through multiple test runs, using the remaining runs for training the model. For each test run, it loads preprocessed fMRI data, either applies a mask, takes the top voxels, or neither of those, and extracts relevant brain responses. These brain responses are used as input features for a machine learning model to predict clip embeddings. The code evaluates the accuracy of the model's predictions by comparing the cosine distances between the ground truth and predicted embeddings. It computes the accuracy, variance, and other statistics, collecting these values for further analysis. The final output includes the mean accuracy, variance, standard deviation, and minimum and maximum accuracy values across all test runs and subjects. The code aims to assess the model's performance in predicting brain responses to visual stimuli. This code especially can be edited to manipulate results, or to try different tests.  \n",
    "In the next two code cells the first one calculates all of the values in one run. This can get the different accuracies of the actual data. While the second code cell calculates all of the values over 9 runs (can easily be changed ~ probably increased), with shuffled data. This is to get the random data to compare to, to see whether or not the actual data is giving meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d40fce5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e5ccf020dd4f69ae54d021155503db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Decoding\\fmri-preprocessing\\tc2see.py:70: RuntimeWarning: invalid value encountered in divide\n",
      "  run_bold = (run_bold - predicted_bold) / group['bold_trend_std'][i]\n",
      "e:\\Decoding\\fmri-preprocessing\\tc2see.py:76: RuntimeWarning: Mean of empty slice\n",
      "  bold = (bold - np.nanmean(bold, axis=0)) / np.nanstd(bold, axis=0)\n",
      "c:\\Users\\James\\anaconda3\\envs\\representationsLab\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa40ccaadc4a472d96bfc6193ad3acba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dab25d0fa0d4d86b8ca2de12198f8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 166634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Decoding\\fmri-preprocessing\\noise_ceiling.py:22: RuntimeWarning: Mean of empty slice\n",
      "  betas_var_mean = np.nanmean(np.stack(betas_var), axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7977, 1.0910, 0.8242,  ..., 0.9765, 0.7973, 0.8976],\n",
      "        [0.9113, 0.8307, 0.8458,  ..., 1.0671, 0.9270, 0.8633],\n",
      "        [0.9988, 0.9060, 0.8498,  ..., 1.0243, 0.9998, 0.9083],\n",
      "        ...,\n",
      "        [0.9447, 1.1194, 0.9651,  ..., 1.1311, 0.9656, 1.0313],\n",
      "        [0.8691, 1.0451, 0.9205,  ..., 1.0467, 0.8793, 0.9136],\n",
      "        [1.1197, 1.0239, 0.9971,  ..., 1.1348, 1.0017, 0.9544]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Decoding\\fmri-preprocessing\\tc2see.py:70: RuntimeWarning: invalid value encountered in divide\n",
      "  run_bold = (run_bold - predicted_bold) / group['bold_trend_std'][i]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 70\u001b[0m\n\u001b[0;32m     60\u001b[0m training_run_ids\u001b[38;5;241m.\u001b[39mremove(test_run_id) \u001b[38;5;66;03m# Remove the test data id \u001b[39;00m\n\u001b[0;32m     62\u001b[0m load_data_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     63\u001b[0m     path \u001b[38;5;241m=\u001b[39m data_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtc2see-v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtc2see_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-bold-test-31.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     64\u001b[0m     subject \u001b[38;5;241m=\u001b[39m subject,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m     interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m bold_train, stimulus_ids_train, mask, affine \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mload_data_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_run_ids\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask[mask] \u001b[38;5;66;03m# flatten mask\u001b[39;00m\n\u001b[0;32m     77\u001b[0m bold_test, stimulus_ids_test, _, _ \u001b[38;5;241m=\u001b[39m load_data(\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mload_data_params,\n\u001b[0;32m     79\u001b[0m     run_ids \u001b[38;5;241m=\u001b[39m [test_run_id]\n\u001b[0;32m     80\u001b[0m )\n",
      "File \u001b[1;32me:\\Decoding\\fmri-preprocessing\\tc2see.py:71\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(path, subject, tr_offset, run_normalize, interpolation, interpolation_order, run_ids)\u001b[0m\n\u001b[0;32m     69\u001b[0m         predicted_bold \u001b[38;5;241m=\u001b[39m trend_coeffs \u001b[38;5;241m@\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold_trend\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[0;32m     70\u001b[0m         run_bold \u001b[38;5;241m=\u001b[39m (run_bold \u001b[38;5;241m-\u001b[39m predicted_bold) \u001b[38;5;241m/\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold_trend_std\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[1;32m---> 71\u001b[0m     bold\u001b[38;5;241m.\u001b[39mappend(run_bold\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     72\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(run_ids)\n\u001b[0;32m     73\u001b[0m bold \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(bold)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def bb_mask(mask, vc_height_min, vc_height_max, vc_width, vc_depth):\n",
    "    brain_width, brain_depth, brain_height = mask.shape\n",
    "    vc_center = np.array([brain_width//2, 0, 0])\n",
    "    vc_bl = vc_center + np.array([-vc_width,0,vc_height_min]) # bottom left\n",
    "    vc_tr = vc_center + np.array([vc_width,vc_depth,vc_height_max]) # top right\n",
    "    vc_mask = np.zeros_like(mask)\n",
    "    vc_mask[vc_bl[0]:vc_tr[0], vc_bl[1]:vc_tr[1], vc_bl[2]:vc_tr[2]] = True # boolean array\n",
    "    return vc_mask[mask] # flattens both 3D arrays into a one dimensional vector (True values inside the bb, False values outside). Intersection of bb and brain\n",
    "\n",
    "accuracies = {}\n",
    "# subjs = [str(sub) if sub >= 10 else '0'+str(sub) for sub in range(1,30)] \n",
    "subjs = ['31']\n",
    "for subj in tqdm(subjs):\n",
    "    # try:\n",
    "        tr = 2 # 1.97\n",
    "        subject_no = subj \n",
    "        subject = f'sub-{subject_no}'\n",
    "\n",
    "        bold, stimulus_ids, mask, affine = load_data(\n",
    "            data_path / f'tc2see-v{tc2see_version}-bold-test-31.hdf5', \n",
    "            subject,\n",
    "            tr_offset=6 / tr,\n",
    "            run_normalize='linear_trend',\n",
    "            interpolation=False,\n",
    "        )\n",
    "\n",
    "        model_name = 'ViT-B=32'\n",
    "        embedding_name = 'embedding' \n",
    "\n",
    "        # load the clip embeddings\n",
    "        with h5py.File(derivatives_path / f'{model_name}-features.hdf5', 'r') as f:\n",
    "            stimulus = f[embedding_name][:]\n",
    "        Y = stimulus[stimulus_ids] # get the stimulus representations to decode\n",
    "\n",
    "\n",
    "        subject = f'sub-{subject_no}'\n",
    "        # 6 Runs - 1 run as the test each time (a run is each time the person gets into the scanner and looks into the scanner for a certain amount of time ~ approx 6 mins)\n",
    "        results = dict\n",
    "        permutation_test = False\n",
    "        nc_threshold = 9\n",
    "        iterations = 1\n",
    "        num_runs = 6\n",
    "\n",
    "        max_tot_acc = 0\n",
    "        threshold_for_max = 0\n",
    "\n",
    "        all_itters_avg = 0\n",
    "        all_itters_var = 0\n",
    "        all_itters_std = 0\n",
    "        all_itters_max = 0\n",
    "        all_itters_min = 0\n",
    "\n",
    "        for iteration in tqdm(range(iterations)):\n",
    "            itter_accuracy = 0\n",
    "            itter_variance = 0\n",
    "            \n",
    "            # Cross validation. Use every id as test data once.\n",
    "            for test_run_id in tqdm(range(num_runs)):\n",
    "                training_run_ids = list(range(num_runs))\n",
    "                training_run_ids.remove(test_run_id) # Remove the test data id \n",
    "\n",
    "                load_data_params = dict(\n",
    "                    path = data_path / f'tc2see-v{tc2see_version}-bold-test-31.hdf5', \n",
    "                    subject = subject,\n",
    "                    tr_offset = num_runs / tr,\n",
    "                    run_normalize='linear_trend',\n",
    "                    interpolation=False,\n",
    "                )\n",
    "\n",
    "                bold_train, stimulus_ids_train, mask, affine = load_data(\n",
    "                    **load_data_params,\n",
    "                    run_ids = training_run_ids\n",
    "                )\n",
    "\n",
    "                mask = mask[mask] # flatten mask\n",
    "\n",
    "                bold_test, stimulus_ids_test, _, _ = load_data(\n",
    "                    **load_data_params,\n",
    "                    run_ids = [test_run_id]\n",
    "                )\n",
    "\n",
    "\n",
    "                # argsort_ids = np.argsort(-nc_vc) # Default ascending, make descending\n",
    "                # argsort_ids = argsort_ids[:5000] # Up to 500 voxels (go about by around powers of 2)\n",
    "                # selection_mask = (nc > nc_threshold) & vc_mask\n",
    "                # print(f'{nc_threshold=}, num_voxels={(nc > nc_threshold).sum()}')\n",
    "                # X_train = bold_train[:, selection_mask] # X's are the brain responses (brain numbers in response to images)  (Within noise ceiling threshold and bounding box)\n",
    "                # X_train = bold_train[:, argsort_ids] # X's are the brain responses (brain numbers in response to images) (With limited voxel amounts)\n",
    "                # X_test = bold_test[:, argsort_ids]\n",
    "                # X_test = bold_test[:, vc_mask]\n",
    "                \n",
    "                print(bold.shape)\n",
    "                ncsnr = compute_ncsnr(bold_train, stimulus_ids_train) # Compute noise ceiling noise ratio\n",
    "                nc = compute_nc(ncsnr, num_averages=1)\n",
    "\n",
    "                nc_vc = nc.copy() \n",
    "                nc_vc[~mask] = 0 # Set values not in mask to zero \n",
    "                argsort_ids = np.argsort(-nc_vc) # Default ascending, make descending \n",
    "                argsort_ids = argsort_ids[:256] \n",
    "                X_train = bold_train[:, argsort_ids]    \n",
    "\n",
    "                # ##################################\n",
    "                # bold_train[:, argsort_ids] = True\n",
    "                # print(\"Number of ones in mask: \", np.count_nonzero(bold_train == 1))\n",
    "                # bold_train[:, ~argsort_ids] = False\n",
    "                # print(\"Number of zeros in mask: \", np.count_nonzero(bold_train == 0))\n",
    "                # ###################################\n",
    "\n",
    "                # flattened_mask = mask[mask]\n",
    "                # X_train = bold_train[:, flattened_mask]\n",
    "                # X_train = X_train[:, nc > nc_threshold] # X's are the brain responses (brain numbers in response to images)\n",
    "\n",
    "                X_nan_train = np.isnan(X_train) # Checks if any not a number values in x and sets those to zero\n",
    "                X_train[X_nan_train] = 0.\n",
    "\n",
    "                # X_test = bold_test[:, flattened_mask]\n",
    "                # X_test = X_test[:, nc > nc_threshold]\n",
    "                X_test = bold_test[:, argsort_ids]\n",
    "                X_nan_test = np.isnan(X_test) # Checks if any not a number values in x and sets those to zero\n",
    "                X_test[X_nan_test] = 0.\n",
    "\n",
    "                with h5py.File(derivatives_path / f'{model_name}-features.hdf5', 'r') as f:\n",
    "                    stimulus = f[embedding_name][:]\n",
    "                Y_train = stimulus[stimulus_ids_train] \n",
    "                Y_test = stimulus[stimulus_ids_test]\n",
    "\n",
    "                if permutation_test:\n",
    "                    ids = np.arange(Y_train.shape[0])\n",
    "                    np.random.shuffle(ids)\n",
    "                    Y_train = Y_train[ids]\n",
    "\n",
    "                model = FracRidgeRegressorCV()\n",
    "                model.fit(X_train, Y_train)\n",
    "                Y_test_pred = model.predict(X_test) # Y_test and Y_test_pred are n x 512 matrics (n is the number of birds).\n",
    "\n",
    "                distances = cosine_distance(\n",
    "                    torch.from_numpy(Y_test[None]).float(), \n",
    "                    torch.from_numpy(Y_test_pred[:, None]).float()\n",
    "                ) # Y_test(1, N, 512) & Y_test_pred(N, 1, 512) converted to pytorch arrays from np\n",
    "                print(distances)\n",
    "                # Chance is 50% (above 50% is good, below not great, if really close ex. 54% or 52%, prove statistically above chance)\n",
    "                accuracy = round(two_versus_two(distances, stimulus_ids=stimulus_ids).item() * 100, 2) \n",
    "                itter_accuracy += accuracy\n",
    "                \n",
    "                #### code here to see if min/max changed in next cell\n",
    "                if accuracy < all_itters_min:\n",
    "                    min = accuracy\n",
    "                if accuracy > all_itters_max:\n",
    "                    max = accuracy\n",
    "\n",
    "                # itter_variance = 0\n",
    "                #### 66.59 is 50 in next cell\n",
    "                # variance = np.mean([(accuracy - 66.5917) ** 2])\n",
    "                # total_variance += variance\n",
    "\n",
    "                # print(f'{test_run_id=}, {accuracy=}')\n",
    "            \n",
    "            all_itters_avg += itter_accuracy\n",
    "\n",
    "            print(f\"Iteration {iteration} avg accuracy: \", itter_accuracy/num_runs)\n",
    "            # variance = total_variance/6\n",
    "            # std_dev = np.sqrt(variance)\n",
    "            # print(\"Standard Dev\", std_dev)\n",
    "\n",
    "            # another level to print total avg and std and min/max\n",
    "        accuracies[subj] = all_itters_avg/(num_runs*iterations)\n",
    "        total_accuracy = all_itters_avg/(num_runs*iterations)\n",
    "        print(\"Total Accuracy: \", total_accuracy)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"There was an error for subject {subj}: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'31': 55.13}\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)\n",
    "# {'30': 50.824999999999996}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f8d12",
   "metadata": {},
   "source": [
    "Running the code:  \n",
    "To run all of the code, the only areas that need to be changed in both files is each subjects = [] line. And the subject that the data is to be found for should be included in the square brackets. For example:  \n",
    "subjects = ['sub-19']  \n",
    "The code cells should be run in order.  \n",
    "Path files could be changed to if different.  \n",
    "\n",
    "The following different lines of these in the Computing Accuracy, Standard Dev, etc. should be commented out, corresponding to each other. The first one is for computing the accuracies without a bounding box mask, the second one is with a bounding box mask, and the third one simply takes the top 256 voxels. (Have only one of the three in each corresponding to each other not commented out when running it).\n",
    "\n",
    "(1) X_train = bold_train[:, nc > nc_threshold] # X's are the brain responses (brain numbers in response to images)  \n",
    "(2) X_train = bold_train[:, selection_mask] # X's are the brain responses (brain numbers in response to images)  (Within noise ceiling threshold and bounding box)  \n",
    "(3) X_train = bold_train[:, argsort_ids] # X's are the brain responses (brain numbers in response to images) (With limited voxel amounts)  \n",
    "\n",
    "(1)\tX_test = bold_test[:, nc > nc_threshold]  \n",
    "(2)\tX_test = bold_test[:, selection_mask]  \n",
    "(3)\tX_test = bold_test[:, argsort_ids]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
