{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfad221c",
   "metadata": {},
   "source": [
    "Importing Libraries:  \n",
    "Python imports and modules that are required that are imported at the start:\n",
    "-\tos, sys, time, numpy (np alias), pandas (pd alias), matplotlib.pyplot (plt alias),  ipywidgets, tqdm.notebook, nibabel, glmsingle, bids, noise_ceiling, tc2see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42692ec7-9a74-481a-9a60-846a52e70f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from tqdm.notebook import tqdm\n",
    "import nibabel as nib\n",
    "# import glmsingle\n",
    "# from glmsingle.glmsingle import GLM_single\n",
    "import bids\n",
    "from bids import BIDSLayout\n",
    "from scipy.ndimage import zoom, binary_dilation\n",
    "import h5py\n",
    "import nibabel as nib\n",
    "from einops import rearrange\n",
    "\n",
    "dir2 = os.path.abspath('..')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)\n",
    "    \n",
    "from tc2see import load_data\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from fracridge import FracRidgeRegressorCV\n",
    "from metrics import (\n",
    "    cosine_distance, squared_euclidean_distance, r2_score, two_versus_two,\n",
    "    two_versus_two_slow\n",
    ")\n",
    "import warnings\n",
    "\n",
    "from noise_ceiling import (\n",
    "    compute_ncsnr,\n",
    "    compute_nc,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c95e0",
   "metadata": {},
   "source": [
    "Setting Paths and Variables:  \n",
    "It sets up various directory paths for loading and saving data:\n",
    "-\ttc2see_version and subject are variables that specify the version of the dataset and the subject being analyzed.\n",
    "-\ttr is the repetition time (time between volume acquisitions in the fMRI data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a523daaa-43f8-436a-bb98-910bfaf23fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = Path('E:\\\\fmri_processing\\\\results')\n",
    "tc2see_version = 3\n",
    "dataset_path = dataset_root\n",
    "derivatives_path = dataset_path / 'derivatives_TC2See'\n",
    "data_path = derivatives_path / 'fmriprep'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe35b5",
   "metadata": {},
   "source": [
    "Loading Data:  \n",
    "The code loads fMRI data and associated information.\n",
    "-\tIt loads a bold dataset, stimulus IDs, a brain mask, and an affine transformation matrix using the load_data function.\n",
    "-\tThe loaded bold data represents the fMRI signal, and its shape is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe873b4-bc22-436c-9d9a-caeaa081a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 163842)\n"
     ]
    }
   ],
   "source": [
    "tr = 2 # 1.97\n",
    "subject_no = '07' # ['06', '15', '19', '21', '22', '08', '09', '11', '12', '16', '24']\n",
    "subject = f'sub-{subject_no}'\n",
    "\n",
    "bold, stimulus_ids, mask, affine = load_data(\n",
    "    data_path / f'tc2see-v{tc2see_version}-fsaverage-surfs-L.hdf5', \n",
    "    subject,\n",
    "    tr_offset=6 / tr,\n",
    "    run_normalize='linear_trend',\n",
    "    interpolation=False,\n",
    ")\n",
    "print(bold.shape) # num stimuli by num voxels\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64d22e",
   "metadata": {},
   "source": [
    "Model and Embedding Information & Loading Stimulus Representations:  \n",
    "The code defines variables for the model name and embedding name. These refer to the neural network model and feature embeddings used for further analysis. The code loads stimulus embeddings from an HDF5 file and assigns it to the variable Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07224f8b-5b6b-4d06-8d49-781b8430ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViT-B=32'\n",
    "embedding_name = 'embedding' \n",
    "\n",
    "# load the clip embeddings\n",
    "with h5py.File(derivatives_path / f'{model_name}-features.hdf5', 'r') as f:\n",
    "    stimulus = f[embedding_name][:]\n",
    "Y = stimulus[stimulus_ids] # get the stimulus representations to decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a9bfd",
   "metadata": {},
   "source": [
    "Machine Learning Analysis:  \n",
    "This code performs a machine learning analysis. It uses FracRidge for decoding the stimulus representations from the fMRI data. It splits the data into training and validation sets using K-fold cross-validation, it fits the model to the training data and predicts stimulus representations on the validation set, it computes distances between the predicted and actual stimulus representations, and it evaluates the accuracy of the model's predictions.  \n",
    "Also performs a noise ceiling analysis, to assess the quality of the fMRI data. This analysis is performed for different runs and components.  \n",
    "X represents the bold signal data obtained from fMRI scans, after applying selection criteria and data preprocessing.  \n",
    "Y represents stimulus related data, which includes embeddings associated with the stimuli presented during the fMRI scans. It serves as the target data for the regression analysis, where the goal is to predict these stimuli related features based on the selected brain activity data in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1904f6b7-007d-4e37-a754-b2e4ad6c20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from fracridge import FracRidgeRegressorCV\n",
    "# from metrics import (\n",
    "#     cosine_distance, squared_euclidean_distance, r2_score, two_versus_two,\n",
    "#     two_versus_two_slow\n",
    "# )\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')\n",
    "# np.seterr(all=\"ignore\")\n",
    "# run_id_groups = [[2],]\n",
    "# tr = 2 # 1.97\n",
    "# subjects = [f'sub-{subject_no}',]\n",
    "\n",
    "# for subject in subjects:\n",
    "#     print(f'{subject=}')\n",
    "    \n",
    "#     for run_ids in run_id_groups:\n",
    "#         bold, stimulus_ids, mask, affine = load_data(\n",
    "#             data_path / f'tc2see-v{tc2see_version}-bold-2.hdf5', \n",
    "#             subject,\n",
    "#             tr_offset=6 / tr,\n",
    "#             run_normalize='linear_trend',\n",
    "#             interpolation=False,\n",
    "#             run_ids=run_ids,\n",
    "#         )\n",
    "\n",
    "#         nc_file = f'noise_ceiling/{subject}/{subject}__run_ids_2__noise-ceiling.nii.gz'\n",
    "#         nc_series = nib.load(data_path / nc_file).get_fdata()\n",
    "#         print(nc_series.shape)\n",
    "#         component = 3 # 6s\n",
    "#         nc_volume = nc_series[..., component]\n",
    "#         nc_volume[:, :, :5] = 0.\n",
    "#         print(\"Number of zeros in nc_volume: \", np.count_nonzero(nc_volume != 0))\n",
    "#         print(mask.shape)\n",
    "#         print(nc_volume.shape)\n",
    "#         print(\"Number of ones in mask: \", np.count_nonzero(mask == 1))\n",
    "#         nc = nc_volume[mask]\n",
    "#         print(np.unique(nc))\n",
    "\n",
    "#         nc.shape\n",
    "#         nc_threshold = 20.\n",
    "#         print(bold.shape)\n",
    "#         X = bold[:, nc > nc_threshold]\n",
    "\n",
    "#         X_nan = np.isnan(X)\n",
    "#         X[X_nan] = 0.\n",
    "        \n",
    "#         with h5py.File(derivatives_path / f'{model_name}-features.hdf5', 'r') as f:\n",
    "#             stimulus = f[embedding_name][:]\n",
    "#         Y = stimulus[stimulus_ids]\n",
    "\n",
    "#         folds = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "#         Y_pred = np.zeros_like(Y)\n",
    "#         for train_ids, val_ids in folds.split(X):\n",
    "#             X_train, Y_train = X[train_ids], Y[train_ids]\n",
    "#             X_val, Y_val = X[val_ids], Y[val_ids]\n",
    "\n",
    "#             model = FracRidgeRegressorCV()\n",
    "#             model.fit(X_train, Y_train)\n",
    "#             Y_pred[val_ids] = model.predict(X_val)\n",
    "\n",
    "#         distances = cosine_distance(torch.from_numpy(Y[None]).float(), torch.from_numpy(Y_pred[:, None]).float())\n",
    "#         accuracy  = round(two_versus_two(distances, stimulus_ids=stimulus_ids).item() * 100, 2) \n",
    "#         accuracy2 = round(two_versus_two_slow(distances, stimulus_ids=stimulus_ids) * 100, 2)\n",
    "\n",
    "#         print(f'\\n---- Run(s): {run_ids} ----')\n",
    "#         print(f'X.shape: ', X.shape)\n",
    "#         print(f'NC Threshold: {nc_threshold}\\nNum Voxels: {(nc > nc_threshold).sum()}\\nAccuracy: {accuracy}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af4e9b",
   "metadata": {},
   "source": [
    "Computing Accuracy, Standard Dev, etc:  \n",
    "This code segment performs a series of operations to evaluate the accuracy of a model's predictions for a given subject. It calculates the accuracy and variance of the model's predictions using a cross-validation approach with a range of parameters. The code initializes various variables, including the subject ID and configuration parameters. It then iterates through multiple test runs, using the remaining runs for training the model. For each test run, it loads preprocessed fMRI data, either applies a mask, takes the top voxels, or neither of those, and extracts relevant brain responses. These brain responses are used as input features for a machine learning model to predict clip embeddings. The code evaluates the accuracy of the model's predictions by comparing the cosine distances between the ground truth and predicted embeddings. It computes the accuracy, variance, and other statistics, collecting these values for further analysis. The final output includes the mean accuracy, variance, standard deviation, and minimum and maximum accuracy values across all test runs and subjects. The code aims to assess the model's performance in predicting brain responses to visual stimuli. This code especially can be edited to manipulate results, or to try different tests.  \n",
    "In the next two code cells the first one calculates all of the values in one run. This can get the different accuracies of the actual data. While the second code cell calculates all of the values over 9 runs (can easily be changed ~ probably increased), with shuffled data. This is to get the random data to compare to, to see whether or not the actual data is giving meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40fce5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5b773a003340a095f5a97bbebb3e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was an error for subject 07:  not enough values to unpack (expected 4, got 2)\n"
     ]
    }
   ],
   "source": [
    "def bb_mask(mask, vc_height_min, vc_height_max, vc_width, vc_depth):\n",
    "    brain_width, brain_depth, brain_height = mask.shape\n",
    "    vc_center = np.array([brain_width//2, 0, 0])\n",
    "    vc_bl = vc_center + np.array([-vc_width,0,vc_height_min]) # bottom left\n",
    "    vc_tr = vc_center + np.array([vc_width,vc_depth,vc_height_max]) # top right\n",
    "    vc_mask = np.zeros_like(mask)\n",
    "    vc_mask[vc_bl[0]:vc_tr[0], vc_bl[1]:vc_tr[1], vc_bl[2]:vc_tr[2]] = True # boolean array\n",
    "    return vc_mask[mask] # flattens both 3D arrays into a one dimensional vector (True values inside the bb, False values outside). Intersection of bb and brain\n",
    "\n",
    "accuracies = {}\n",
    "# subjs = [str(sub) if sub >= 10 else '0'+str(sub) for sub in range(1,30)] \n",
    "subjs = ['15']\n",
    "for subj in tqdm(subjs):\n",
    "    try:\n",
    "        tr = 2 # 1.97\n",
    "        subject_no = subj \n",
    "        subject = f'sub-{subject_no}'\n",
    "\n",
    "        bold, stimulus_ids, mask, affine = load_data(\n",
    "            data_path / f'tc2see-v{tc2see_version}-fsaverage-surfs-L.hdf5', \n",
    "            subject,\n",
    "            tr_offset=6 / tr,\n",
    "            run_normalize='linear_trend',\n",
    "            interpolation=False,\n",
    "        )\n",
    "\n",
    "        model_name = 'ViT-B=32'\n",
    "        embedding_name = 'embedding' \n",
    "\n",
    "        # load the clip embeddings\n",
    "        with h5py.File(derivatives_path / f'{model_name}-features.hdf5', 'r') as f:\n",
    "            stimulus = f[embedding_name][:]\n",
    "        Y = stimulus[stimulus_ids] # get the stimulus representations to decode\n",
    "\n",
    "\n",
    "        subject = f'sub-{subject_no}'\n",
    "        # 6 Runs - 1 run as the test each time (a run is each time the person gets into the scanner and looks into the scanner for a certain amount of time ~ approx 6 mins)\n",
    "        results = dict\n",
    "        permutation_test = False\n",
    "        nc_threshold = 9\n",
    "        iterations = 1\n",
    "        num_runs = 6\n",
    "\n",
    "        max_tot_acc = 0\n",
    "        threshold_for_max = 0\n",
    "\n",
    "        all_itters_avg = 0\n",
    "        all_itters_var = 0\n",
    "        all_itters_std = 0\n",
    "        all_itters_max = 0\n",
    "        all_itters_min = 0\n",
    "\n",
    "        for iteration in tqdm(range(iterations)):\n",
    "            itter_accuracy = 0\n",
    "            itter_variance = 0\n",
    "            \n",
    "            # Cross validation. Use every id as test data once.\n",
    "            for test_run_id in tqdm(range(num_runs)):\n",
    "                training_run_ids = list(range(num_runs))\n",
    "                training_run_ids.remove(test_run_id) # Remove the test data id \n",
    "\n",
    "                load_data_params = dict(\n",
    "                    path = data_path / f'tc2see-v{tc2see_version}-fsaverage-surfs-L.hdf5', \n",
    "                    subject = subject,\n",
    "                    tr_offset = num_runs / tr,\n",
    "                    run_normalize='linear_trend',\n",
    "                    interpolation=False,\n",
    "                )\n",
    "\n",
    "                bold_train, stimulus_ids_train, mask, affine = load_data(\n",
    "                    **load_data_params,\n",
    "                    run_ids = training_run_ids\n",
    "                )\n",
    "\n",
    "                # vc_mask = bb_mask(mask, 16, 55, 31, 36)\n",
    "\n",
    "                bold_test, stimulus_ids_test, _, _ = load_data(\n",
    "                    **load_data_params,\n",
    "                    run_ids = [test_run_id]\n",
    "                )\n",
    "\n",
    "\n",
    "                # argsort_ids = np.argsort(-nc_vc) # Default ascending, make descending\n",
    "                # argsort_ids = argsort_ids[:5000] # Up to 500 voxels (go about by around powers of 2)\n",
    "                # selection_mask = (nc > nc_threshold) & vc_mask\n",
    "                # print(f'{nc_threshold=}, num_voxels={(nc > nc_threshold).sum()}')\n",
    "                # X_train = bold_train[:, selection_mask] # X's are the brain responses (brain numbers in response to images)  (Within noise ceiling threshold and bounding box)\n",
    "                # X_train = bold_train[:, argsort_ids] # X's are the brain responses (brain numbers in response to images) (With limited voxel amounts)\n",
    "                # X_test = bold_test[:, argsort_ids]\n",
    "                # X_test = bold_test[:, vc_mask]\n",
    "\n",
    "                \n",
    "                ncsnr = compute_ncsnr(bold_train, stimulus_ids_train) # Compute noise ceiling noise ratio\n",
    "                nc = compute_nc(ncsnr, num_averages=1)\n",
    "\n",
    "                # nc_vc = nc.copy() ##\n",
    "                # nc_vc[~vc_mask] = 0 # Set values not in mask to zero ##\n",
    "                argsort_ids = np.argsort(-nc) # Default ascending, make descending ##\n",
    "                argsort_ids = argsort_ids[:256] ##\n",
    "                X_train = bold_train[:, argsort_ids] ##\n",
    "\n",
    "                # ##################################\n",
    "                # bold_train[:, argsort_ids] = True\n",
    "                # print(\"Number of ones in mask: \", np.count_nonzero(bold_train == 1))\n",
    "                # bold_train[:, ~argsort_ids] = False\n",
    "                # print(\"Number of zeros in mask: \", np.count_nonzero(bold_train == 0))\n",
    "                # ###################################\n",
    "\n",
    "                # flattened_mask = mask[mask]\n",
    "                # X_train = bold_train[:, flattened_mask]\n",
    "                # X_train = X_train[:, nc > nc_threshold] # X's are the brain responses (brain numbers in response to images)\n",
    "\n",
    "                X_nan_train = np.isnan(X_train) # Checks if any not a number values in x and sets those to zero\n",
    "                X_train[X_nan_train] = 0.\n",
    "\n",
    "                # X_test = bold_test[:, flattened_mask]\n",
    "                # X_test = X_test[:, nc > nc_threshold]\n",
    "                X_test = bold_test[:, argsort_ids]\n",
    "                X_nan_test = np.isnan(X_test) # Checks if any not a number values in x and sets those to zero\n",
    "                X_test[X_nan_test] = 0.\n",
    "\n",
    "                with h5py.File(derivatives_path / f'{model_name}-features.hdf5', 'r') as f:\n",
    "                    stimulus = f[embedding_name][:]\n",
    "                Y_train = stimulus[stimulus_ids_train] \n",
    "                Y_test = stimulus[stimulus_ids_test]\n",
    "\n",
    "                if permutation_test:\n",
    "                    ids = np.arange(Y_train.shape[0])\n",
    "                    np.random.shuffle(ids)\n",
    "                    Y_train = Y_train[ids]\n",
    "\n",
    "                model = FracRidgeRegressorCV()\n",
    "                model.fit(X_train, Y_train)\n",
    "                Y_test_pred = model.predict(X_test) # Y_test and Y_test_pred are n x 512 matrics (n is the number of birds).\n",
    "\n",
    "                distances = cosine_distance(\n",
    "                    torch.from_numpy(Y_test[None]).float(), \n",
    "                    torch.from_numpy(Y_test_pred[:, None]).float()\n",
    "                ) # Y_test(1, N, 512) & Y_test_pred(N, 1, 512) converted to pytorch arrays from np\n",
    "\n",
    "                # Chance is 50% (above 50% is good, below not great, if really close ex. 54% or 52%, prove statistically above chance)\n",
    "                accuracy = round(two_versus_two(distances, stimulus_ids=stimulus_ids).item() * 100, 2) \n",
    "                \n",
    "                #### code here to see if min/max changed in next cell\n",
    "                itter_accuracy += accuracy\n",
    "                if accuracy < all_itters_min:\n",
    "                    min = accuracy\n",
    "                if accuracy > all_itters_max:\n",
    "                    max = accuracy\n",
    "\n",
    "                # itter_variance = 0\n",
    "                #### 66.59 is 50 in next cell\n",
    "                # variance = np.mean([(accuracy - 66.5917) ** 2])\n",
    "                # total_variance += variance\n",
    "\n",
    "                # print(f'{test_run_id=}, {accuracy=}')\n",
    "            \n",
    "            all_itters_avg += itter_accuracy\n",
    "\n",
    "            print(f\"Iteration {iteration} avg accuracy: \", itter_accuracy/num_runs)\n",
    "            # variance = total_variance/6\n",
    "            # std_dev = np.sqrt(variance)\n",
    "            # print(\"Standard Dev\", std_dev)\n",
    "\n",
    "            # another level to print total avg and std and min/max\n",
    "        accuracies[subj] = all_itters_avg/(num_runs*iterations)\n",
    "        total_accuracy = all_itters_avg/(num_runs*iterations)\n",
    "        print(\"Total Accuracy: \", total_accuracy)\n",
    "    except Exception as e:\n",
    "        print(f\"There was an error for subject {subj}: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec92716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'15': 68.35833333333333}\n"
     ]
    }
   ],
   "source": [
    "print(accuracies)\n",
    "# {'30': 50.824999999999996}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183f8d12",
   "metadata": {},
   "source": [
    "Running the code:  \n",
    "To run all of the code, the only areas that need to be changed in both files is each subjects = [] line. And the subject that the data is to be found for should be included in the square brackets. For example:  \n",
    "subjects = ['sub-19']  \n",
    "The code cells should be run in order.  \n",
    "Path files could be changed to if different.  \n",
    "\n",
    "The following different lines of these in the Computing Accuracy, Standard Dev, etc. should be commented out, corresponding to each other. The first one is for computing the accuracies without a bounding box mask, the second one is with a bounding box mask, and the third one simply takes the top 256 voxels. (Have only one of the three in each corresponding to each other not commented out when running it).\n",
    "\n",
    "(1) X_train = bold_train[:, nc > nc_threshold] # X's are the brain responses (brain numbers in response to images)  \n",
    "(2) X_train = bold_train[:, selection_mask] # X's are the brain responses (brain numbers in response to images)  (Within noise ceiling threshold and bounding box)  \n",
    "(3) X_train = bold_train[:, argsort_ids] # X's are the brain responses (brain numbers in response to images) (With limited voxel amounts)  \n",
    "\n",
    "(1)\tX_test = bold_test[:, nc > nc_threshold]  \n",
    "(2)\tX_test = bold_test[:, selection_mask]  \n",
    "(3)\tX_test = bold_test[:, argsort_ids]  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
