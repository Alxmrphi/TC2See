{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "760e725a",
   "metadata": {},
   "source": [
    "Importing Libraries:  \n",
    "Python imports and modules that are required that are imported at the start:  \n",
    "-\tos, sys, time, numpy (np alias), pandas (pd alias), matplotlib.pyplot (plt alias) ipywidgets, tqdm.notebook, nibabel, glmsingle, bids, noise_ceiling and tc2see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d455321-5f55-437b-b90d-114983b98c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from tqdm.notebook import tqdm\n",
    "import h5py\n",
    "\n",
    "import nibabel as nib\n",
    "from nilearn import surface\n",
    "import bids\n",
    "from bids import BIDSLayout\n",
    "\n",
    "dir2 = os.path.abspath('../')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: \n",
    "    sys.path.append(dir1)\n",
    "    \n",
    "from noise_ceiling import (\n",
    "    compute_ncsnr,\n",
    "    compute_nc,\n",
    ")\n",
    "\n",
    "from tc2see import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda9559",
   "metadata": {},
   "source": [
    "Defining Dataset Paths and Variables:  \n",
    "These sections focuses on establishing the paths to project directories and initializing key variables. It defines the paths to the dataset, derivatives, and preprocessed fMRI data, setting up the ways to access project data. Additionally, variables related to the dataset version, the number of runs, and the task specifications are set within this section.  \n",
    "(Adjust this path and any other path as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854d408-ae78-4360-991f-6d0d84b0ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = Path('E:\\\\fmri_processing\\\\results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22df9105-c0b4-4a2a-8513-8596d337f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc2see_version = 3 # [1, 2]\n",
    "dataset_path = dataset_root\n",
    "derivatives_path = dataset_path / 'derivatives_TC2See'\n",
    "data_path = derivatives_path / 'fmriprep'\n",
    "num_runs = 6 if tc2see_version in (1, 3) else 8\n",
    "\n",
    "# Initialize BIDSLayouts for querying files.\n",
    "dataset_layout = BIDSLayout(dataset_path / 'TC2See')\n",
    "derivatives_layout = BIDSLayout(derivatives_path / 'fmriprep', derivatives=True, validate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe9027",
   "metadata": {},
   "source": [
    "Processing fMRI Data for Subjects:  \n",
    "This code segment focuses on configuring and preparing the environment for the analysis of the preprocessed fMRI data. First it initializes configuration variables such as subject IDs, TR duration, brain mask dilation parameters, and the number of stimuli. Then, it loads stimulus images and creates a mapping of stimulus names to unique identifiers. The code then creates an HDF5 file for storing the preprocessed fMRI data, with the filename derived from the specified version. For each subject, it initializes a group within the HDF5 file and manages the loading of the brain mask, potentially applying binary dilation if required. Various datasets within the subject's group are created to store bold data, statistics, trends, and stimulus related information. This segment ensures that the preprocessed fMRI data is well organized and structured for further analysis and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c50f4-2504-4e4a-be93-bac60cb66740",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"bird\"\n",
    "space = 'fsaverage' \n",
    "\n",
    "# subjects = [ '0'+str(num) if num < 10 else str(num) for num in range(5,35)]\n",
    "subjects = ['18']\n",
    "\n",
    "tr = 2. # 1.97  # TR duration (in seconds)\n",
    "mask_dilations = 3  # Number of dilation iterations for the brain mask\n",
    "num_stimuli = 75 # 112  # Total number of different stimuli\n",
    "\n",
    "# Load stimulus images and create a mapping of stimulus names to unique identifiers\n",
    "stimulus_images = h5py.File(derivatives_path / 'stimulus-images.hdf5', 'r')\n",
    "stimulus_id_map = {name: i for i, name in enumerate(stimulus_images.attrs['stimulus_names'])}\n",
    " \n",
    "new_or_append = 'w' # Use 'a' for append/overwrite, 'w' for new hdf5 file\n",
    "           \n",
    "# Create or append to an HDF5 file to store preprocessed fMRI data\n",
    "with h5py.File(data_path / f'tc2see-v{tc2see_version}-fsaverage-surf_17.hdf5', new_or_append) as f:\n",
    "    for subject in tqdm(subjects):\n",
    "        if f'sub-{subject}' not in list(f.keys()):\n",
    "            try:\n",
    "                print(f\"Processing subject {subject}...\")\n",
    "                group = f.require_group(f'sub-{subject}')\n",
    "\n",
    "                fsaverage_surf_list = []\n",
    "                for hemi in ('L', 'R'):\n",
    "                    \n",
    "                    leftOrRight = 0 if hemi == 'L' else 1\n",
    "                \n",
    "                    fsaverage_surf_hemi = derivatives_layout.get(\n",
    "                            subject=subject,\n",
    "                            run=1,\n",
    "                            task=task,\n",
    "                            space=space, \n",
    "                            extension='func.gii',\n",
    "                    )[leftOrRight]\n",
    "\n",
    "                    fsaverage_surf_hemi = surface.load_surf_data(fsaverage_surf_hemi).astype(np.float64)\n",
    "                    fsaverage_surf_list.append(fsaverage_surf_hemi)\n",
    "\n",
    "                fsaverage_surf = np.concatenate(fsaverage_surf_list, axis=0)\n",
    "\n",
    "                num_voxels = fsaverage_surf.shape[0]\n",
    "                num_trs = fsaverage_surf.shape[1]\n",
    "\n",
    "                group.require_dataset('bold', shape=(num_runs, num_trs, num_voxels), dtype='f4')\n",
    "                group.require_dataset('bold_mean', shape=(num_runs, num_voxels), dtype='f4')\n",
    "                group.require_dataset('bold_std', shape=(num_runs, num_voxels), dtype='f4')\n",
    "                group.require_dataset('bold_trend', shape=(num_runs, 2, num_voxels), dtype='f4')\n",
    "                group.require_dataset('bold_trend_std', shape=(num_runs, num_voxels), dtype='f4')\n",
    "                group.require_dataset('stimulus_trs', shape=(num_runs, num_stimuli), dtype='f4')\n",
    "                group.require_dataset('stimulus_ids', shape=(num_runs, num_stimuli), dtype='i4')\n",
    "                \n",
    "                for run_id in tqdm(range(num_runs)):\n",
    "                    \n",
    "                    fsaverage_surf_list = []\n",
    "                    for hemi in ('L', 'R'):\n",
    "                        \n",
    "                        leftOrRight = 0 if hemi == 'L' else 1\n",
    "                    \n",
    "                        fsaverage_surf_hemi = derivatives_layout.get(\n",
    "                                subject=subject,\n",
    "                                run=run_id + 1,\n",
    "                                task=task,\n",
    "                                space=space, \n",
    "                                extension='func.gii',\n",
    "                        )[leftOrRight]\n",
    "\n",
    "                        fsaverage_surf_hemi = surface.load_surf_data(fsaverage_surf_hemi).astype(np.float64)\n",
    "                        fsaverage_surf_list.append(fsaverage_surf_hemi)\n",
    "\n",
    "                    fsaverage_surf = np.concatenate(fsaverage_surf_list, axis=0) # (327684, 231)\n",
    "                    fsaverage_surf = np.transpose(fsaverage_surf) # (231, 327684)\n",
    "                    np.save(\"mean_fsaverage_surf\", np.mean(fsaverage_surf, axis=0))\n",
    "                    \n",
    "                    num_trs_run = fsaverage_surf.shape[0]\n",
    "\n",
    "                    trend_coeffs = np.stack([np.arange(num_trs_run), np.ones(shape=num_trs_run)], axis=1) # (231, 2)\n",
    "                    \n",
    "                    # Perform linear detrending on the bold data\n",
    "                    bold_trend = np.linalg.lstsq(trend_coeffs, fsaverage_surf, rcond=None)[0] # (2, 327684)\n",
    "                    bold_predicted = trend_coeffs @ bold_trend # (231, 327684)\n",
    "                    np.save(\"bold_predicted\", bold_predicted) \n",
    "                    \n",
    "                    bold_detrend = fsaverage_surf - bold_predicted # (231, 327684)\n",
    "                    np.save(\"bold_detrend\", bold_detrend)\n",
    "\n",
    "                    np.save(\"mean_bold_detrend\", np.mean(bold_detrend, axis=0))\n",
    "\n",
    "                    # Load events data for the current subject and run\n",
    "                    events_file = dataset_layout.get(\n",
    "                        subject=subject,\n",
    "                        run=run_id + 1,\n",
    "                        task=task,\n",
    "                        extension='tsv'\n",
    "                    )[0]\n",
    "                    \n",
    "                    events_df = pd.read_csv(events_file.path, sep='\\t')\n",
    "                    events_df = events_df[events_df['stimulus'] != '+']\n",
    "                    stimulus_names = [Path(stimulus_path).stem for stimulus_path in events_df['stimulus']]\n",
    "                    stimulus_names = [\n",
    "                        name[:name.find('hash')-1] if \"hash\" in name else name\n",
    "                        for name in stimulus_names\n",
    "                    ]\n",
    "                    stimulus_ids = [stimulus_id_map[name] for name in stimulus_names]\n",
    "                    \n",
    "                    stimulus_trs = np.array(events_df['tr']).astype(np.float32)\n",
    "                    \n",
    "                    # Store various datasets in the HDF5 file\n",
    "                    group['bold'][run_id, :num_trs_run] = fsaverage_surf\n",
    "                    group['bold_mean'][run_id] = fsaverage_surf.mean(axis=0)\n",
    "                    group['bold_std'][run_id] = fsaverage_surf.std(axis=0)\n",
    "                    group['bold_trend'][run_id] = bold_trend\n",
    "                    group['bold_trend_std'][run_id] = bold_detrend.std(axis=0)\n",
    "                    group['stimulus_trs'][run_id] = stimulus_trs\n",
    "                    group['stimulus_ids'][run_id] = stimulus_ids\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {subject}: {e}\")\n",
    "                del f[f'sub-{subject}']\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Subject {subject} already exists\")\n",
    "            print(f[f'sub-{subject}']['bold'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
